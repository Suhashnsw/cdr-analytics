package com.devinline.spark.SparkSample

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.rdd.RDD.rddToPairRDDFunctions

class CSVWordCount {
  def main(args: Array[String]) = {
    
    //Start the Spark context
    val conf = new SparkConf()
      .setAppName("CSVWordCount")
      .setMaster("local")
    val sc = new SparkContext(conf)
    
    //Read some example file to a test RDD
    val csvFile = sc.textFile("../../../Exercises/sales_log/sales_log/sales-1.csv")
    
    csvFile.distinct()// remove all duplicate key;value pairs
        .map(line => line.split(";")(6)) // extract the keys
        .map(k =>  (k,1)) // convert to countable tuples
        .reduceByKey(_+_) // count keys
        .saveAsTextFile("Output.txt") //Save to a text file
    
    sc.stop()    
  }
}